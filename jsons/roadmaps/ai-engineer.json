{
  "ai-engineer": "---\njsonUrl: '/jsons/roadmaps/ai-engineer.json'\npdfUrl: '/pdfs/roadmaps/ai-engineer.pdf'\norder: 4\nrenderer: 'editor'\nbriefTitle: 'AI Engineer'\nbriefDescription: 'Step by step guide to becoming an AI Engineer in 2025'\ntitle: 'AI Engineer'\ndescription: 'Step by step guide to becoming an AI Engineer in 2025'\nhasTopics: true\nisNew: false\ndimensions:\n  width: 968\n  height: 3200\nquestion:\n  title: 'What is an AI Engineer?'\n  description: |\n    An AI Engineer uses pre-trained models and existing AI tools to improve user experiences. They focus on applying AI in practical ways, without building models from scratch. This is different from AI Researchers and ML Engineers, who focus more on creating new models or developing AI theory.\nschema:\n  headline: 'AI Engineer Roadmap'\n  description: 'Learn how to become an AI Engineer with this interactive step by step guide in 2023. We also have resources and short descriptions attached to the roadmap items so you can get everything you want to learn in one place.'\n  imageUrl: 'https://roadmap.sh/roadmaps/ai-engineer.png'\n  datePublished: '2024-10-03'\n  dateModified: '2024-10-03'\nseo:\n  title: 'AI Engineer Roadmap'\n  description: 'Learn to become an AI Engineer using this roadmap. Community driven, articles, resources, guides, interview questions, quizzes for modern AI development.'\n  keywords:\n    - 'ai engineer roadmap 2024'\n    - 'ai engineer roadmap 2025'\n    - 'guide to becoming an ai engineer'\n    - 'ai engineer roadmap'\n    - 'ai engineer skills'\n    - 'become an ai engineer'\n    - 'ai engineer career path'\n    - 'skills for ai engineer'\n    - 'ai engineer quiz'\n    - 'ai engineer interview questions'\n    - 'ai engineer learning path'\n    - 'how to become ai engineer 2025'\n    - 'ai engineering concepts'\n    - 'ai engineering curriculum'\n    - 'ai engineer future'\n    - 'ai engineer requirements'\n    - 'machine learning engineer roadmap'\n    - 'generative ai engineer'\n    - 'python for ai engineer'\n    - 'cloud ai services 2025'\n    - 'data engineering for ai'\nrelatedRoadmaps:\n  - 'ai-data-scientist'\n  - 'prompt-engineering'\n  - 'data-analyst'\n  - 'python'\nsitemap:\n  priority: 1\n  changefreq: 'monthly'\ntags:\n  - 'roadmap'\n  - 'main-sitemap'\n  - 'role-roadmap'\n---\n",
  "content": {
    "adding-end-user-ids-in-prompts@4Q5x2VCXedAWISBXUIyin": "# Adding end-user IDs in prompts\n\nSending end-user IDs in your requests can be a useful tool to help OpenAI monitor and detect abuse. This allows OpenAI to provide your team with more actionable feedback in the event that we detect any policy violations in your application.\n\nVisit the following resources to learn more:\n\n- [@official@Sending End-user IDs - OpenAI](https://platform.openai.com/docs/guides/safety-best-practices/end-user-ids)\n",
    "agents-usecases@778HsQzTuJ_3c9OSn5DmH": "# Agents Usecases\n\nAI Agents have a variety of usecases ranging from customer support, workflow automation, cybersecurity, finance, marketing and sales, and more.\n\nVisit the following resources to learn more:\n\n- [@article@Top 15 Use Cases Of AI Agents In Business](https://www.ampcome.com/post/15-use-cases-of-ai-agents-in-business)\n- [@article@A Brief Guide on AI Agents: Benefits and Use Cases](https://www.codica.com/blog/brief-guide-on-ai-agents/)\n- [@video@The Complete Guide to Building AI Agents for Beginners](https://youtu.be/MOyl58VF2ak?si=-QjRD_5y3iViprJX)\n",
    "ai-agents@9XCxilAQ7FRet7lHQr1gE": "# AI Agents\n\nIn AI engineering, \"agents\" refer to autonomous systems or components that can perceive their environment, make decisions, and take actions to achieve specific goals. Agents often interact with external systems, users, or other agents to carry out complex tasks. They can vary in complexity, from simple rule-based bots to sophisticated AI-powered agents that leverage machine learning models, natural language processing, and reinforcement learning.\n\nVisit the following resources to learn more:\n\n- [@article@Building an AI Agent Tutorial - LangChain](https://python.langchain.com/docs/tutorials/agents/)\n- [@article@AI Agents and Their Types](https://play.ht/blog/ai-agents-use-cases/)\n- [@video@The Complete Guide to Building AI Agents for Beginners](https://youtu.be/MOyl58VF2ak?si=-QjRD_5y3iViprJX)\n",
    "ai-agents@AeHkNU-uJ_gBdo5-xdpEu": "# AI Agents\n\nIn AI engineering, \"agents\" refer to autonomous systems or components that can perceive their environment, make decisions, and take actions to achieve specific goals. Agents often interact with external systems, users, or other agents to carry out complex tasks. They can vary in complexity, from simple rule-based bots to sophisticated AI-powered agents that leverage machine learning models, natural language processing, and reinforcement learning.\n\nVisit the following resources to learn more:\n\n- [@article@Building an AI Agent Tutorial - LangChain](https://python.langchain.com/docs/tutorials/agents/)\n- [@article@AI agents and their types](https://play.ht/blog/ai-agents-use-cases/)\n- [@video@The Complete Guide to Building AI Agents for Beginners](https://youtu.be/MOyl58VF2ak?si=-QjRD_5y3iViprJX)",
    "ai-code-editors@XcKeQfpTA5ITgdX51I4y-": "# AI Code Editors\n\nAI code editors are development tools that leverage artificial intelligence to assist software developers in writing, debugging, and optimizing code. These editors go beyond traditional syntax highlighting and code completion by incorporating machine learning models, natural language processing, and data analysis to understand code context, generate suggestions, and even automate portions of the software development process.\n\nVisit the following resources to learn more:\n\n- [@website@Cursor - The AI Code Editor](https://www.cursor.com/)\n- [@website@PearAI - The Open Source, Extendable AI Code Editor](https://trypear.ai/)\n- [@website@Bolt - Prompt, run, edit, and deploy full-stack web apps](https://bolt.new)\n- [@website@Replit - Build Apps using AI](https://replit.com/ai)\n- [@website@v0 - Build Apps with AI](https://v0.dev)\n",
    "ai-engineer-vs-ml-engineer@jSZ1LhPdhlkW-9QJhIvFs": "# AI Engineer vs ML Engineer\n\nAn AI Engineer uses pre-trained models and existing AI tools to improve user experiences. They focus on applying AI in practical ways, without building models from scratch. This is different from AI Researchers and ML Engineers, who focus more on creating new models or developing AI theory.\n\nLearn more from the following resources:\n\n- [@article@What does an AI Engineer do?](https://www.codecademy.com/resources/blog/what-does-an-ai-engineer-do/)\n- [@article@What is an ML Engineer?](https://www.coursera.org/articles/what-is-machine-learning-engineer)\n- [@video@AI vs ML](https://www.youtube.com/watch?v=4RixMPF4xis)\n",
    "ai-safety-and-ethics@8ndKHDJgL_gYwaXC7XMer": "# AI Safety and Ethics\n\nAI safety and ethics involve establishing guidelines and best practices to ensure that artificial intelligence systems are developed, deployed, and used in a manner that prioritizes human well-being, fairness, and transparency. This includes addressing risks such as bias, privacy violations, unintended consequences, and ensuring that AI operates reliably and predictably, even in complex environments. Ethical considerations focus on promoting accountability, avoiding discrimination, and aligning AI systems with human values and societal norms. Frameworks like explainability, human-in-the-loop design, and robust monitoring are often used to build systems that not only achieve technical objectives but also uphold ethical standards and mitigate potential harms.\n\nLearn more from the following resources:\n\n- [@video@What is AI Ethics?](https://www.youtube.com/watch?v=aGwYtUzMQUk)\n- [@article@Understanding Artificial Intelligence Ethics and Safety](https://www.turing.ac.uk/news/publications/understanding-artificial-intelligence-ethics-and-safety)",
    "ai-vs-agi@5QdihE1lLpMc3DFrGy46M": "# AI vs AGI\n\nAI (Artificial Intelligence) refers to systems designed to perform specific tasks by mimicking aspects of human intelligence, such as pattern recognition, decision-making, and language processing. These systems, known as \"narrow AI,\" are highly specialized, excelling in defined areas like image classification or recommendation algorithms but lacking broader cognitive abilities. In contrast, AGI (Artificial General Intelligence) represents a theoretical form of intelligence that possesses the ability to understand, learn, and apply knowledge across a wide range of tasks at a human-like level. AGI would have the capacity for abstract thinking, reasoning, and adaptability similar to human cognitive abilities, making it far more versatile than today’s AI systems. While current AI technology is powerful, AGI remains a distant goal and presents complex challenges in safety, ethics, and technical feasibility.\n\nLearn more from the following resources:\n\n- [@article@What is AGI?](https://aws.amazon.com/what-is/artificial-general-intelligence/)\n- [@article@The crucial difference between AI and AGI](https://www.forbes.com/sites/bernardmarr/2024/05/20/the-crucial-difference-between-ai-and-agi/)",
    "anomaly-detection@AglWJ7gb9rTT2rMkstxtk": "# Anomaly Detection\n\nAnomaly detection with embeddings works by transforming data, such as text, images, or time-series data, into vector representations that capture their patterns and relationships. In this high-dimensional space, similar data points are positioned close together, while anomalies stand out as those that deviate significantly from the typical distribution. This approach is highly effective for detecting outliers in tasks like fraud detection, network security, and quality control.\n\nLearn more from the following resources:\n\n- [@article@Anomaly in Embeddings](https://ai.google.dev/gemini-api/tutorials/anomaly_detection)\n",
    "anthropics-claude@hy6EyKiNxk1x84J63dhez": "# Anthropic's Claude\n\nAnthropic's Claude is an AI language model designed to facilitate safe and scalable AI systems. Named after Claude Shannon, the father of information theory, Claude focuses on responsible AI use, emphasizing safety, alignment with human intentions, and minimizing harmful outputs. Built as a competitor to models like OpenAI's GPT, Claude is designed to handle natural language tasks such as generating text, answering questions, and supporting conversations, with a strong focus on aligning AI behavior with user goals while maintaining transparency and avoiding harmful biases.\n\nLearn more from the following resources:\n\n- [@official@Claude](https://claude.ai)\n- [@video@How To Use Claude Pro For Beginners](https://www.youtube.com/watch?v=J3X_JWQkvo8)\n",
    "audio-processing@mxQYB820447DC6kogyZIL": "# Audio Processing\n\nAudio processing in multimodal AI enables a wide range of use cases by combining sound with other data types, such as text, images, or video, to create more context-aware systems. Use cases include speech recognition paired with real-time transcription and visual analysis in meetings or video conferencing tools, voice-controlled virtual assistants that can interpret commands in conjunction with on-screen visuals, and multimedia content analysis where audio and visual elements are analyzed together for tasks like content moderation or video indexing.\n\nLearn more from the following resources:\n\n- [@article@The State of Audio Processing](https://appwrite.io/blog/post/state-of-audio-processing)\n- [@video@Audio Signal Processing for Machine Learning](https://www.youtube.com/watch?v=iCwMQJnKk2c)",
    "aws-sagemaker@OkYO-aSPiuVYuLXHswBCn": "# AWS SageMaker\n\nAWS SageMaker is a fully managed machine learning service from Amazon Web Services that enables developers and data scientists to build, train, and deploy machine learning models at scale. It provides an integrated development environment, simplifying the entire ML workflow, from data preparation and model development to training, tuning, and inference. SageMaker supports popular ML frameworks like TensorFlow, PyTorch, and Scikit-learn, and offers features like automated model tuning, model monitoring, and one-click deployment. It's designed to make machine learning more accessible and scalable, even for large enterprise applications.\n\nLearn more from the following resources:\n\n- [@official@AWS SageMaker](https://aws.amazon.com/sagemaker/)\n- [@video@Introduction to Amazon SageMaker](https://www.youtube.com/watch?v=Qv_Tr_BCFCQ)",
    "azure-ai@3PQVZbcr4neNMRr6CuNzS": "# Azure AI\n\nAzure AI is a suite of AI services and tools provided by Microsoft through its Azure cloud platform. It includes pre-built AI models for natural language processing, computer vision, and speech, as well as tools for developing custom machine learning models using services like Azure Machine Learning. Azure AI enables developers to integrate AI capabilities into applications with APIs for tasks like sentiment analysis, image recognition, and language translation. It also supports responsible AI development with features for model monitoring, explainability, and fairness, aiming to make AI accessible, scalable, and secure across industries.\n\nLearn more from the following resources:\n\n- [@official@Azure AI](https://azure.microsoft.com/en-gb/solutions/ai)\n- [@video@How to Choose the Right Models for Your Apps](https://www.youtube.com/watch?v=sx_uGylH8eg)",
    "benefits-of-pre-trained-models@1Ga6DbOPc6Crz7ilsZMYy": "# Benefits of Pre-trained Models\n\nPre-trained models offer several benefits in AI engineering by significantly reducing development time and computational resources because these models are trained on large datasets and can be fine-tuned for specific tasks, which enables quicker deployment and better performance with less data. They help overcome the challenge of needing vast amounts of labeled data and computational power for training from scratch. Additionally, pre-trained models often demonstrate improved accuracy, generalization, and robustness across different tasks, making them ideal for applications in natural language processing, computer vision, and other AI domains.\n\nLearn more from the following resources:\n\n- [@article@Why Pre-Trained Models Matter For Machine Learning](https://www.ahead.com/resources/why-pre-trained-models-matter-for-machine-learning/)\n- [@article@Why You Should Use Pre-Trained Models Versus Building Your Own](https://cohere.com/blog/pre-trained-vs-in-house-nlp-models)",
    "bias-and-fairness@lhIU0ulpvDAn1Xc3ooYz_": "# Bias and Fairness\n\nBias and fairness in AI refer to the challenges of ensuring that machine learning models do not produce discriminatory or skewed outcomes. Bias can arise from imbalanced training data, flawed assumptions, or biased algorithms, leading to unfair treatment of certain groups based on race, gender, or other factors. Fairness aims to address these issues by developing techniques to detect, mitigate, and prevent biases in AI systems. Ensuring fairness involves improving data diversity, applying fairness constraints during model training, and continuously monitoring models in production to avoid unintended consequences, promoting ethical and equitable AI use.\n\nLearn more from the following resources:\n\n- [@article@What Do We Do About the Biases in AI?](https://hbr.org/2019/10/what-do-we-do-about-the-biases-in-ai)\n- [@article@AI Bias - What Is It and How to Avoid It?](https://levity.ai/blog/ai-bias-how-to-avoid)\n- [@article@What about fairness, bias and discrimination?](https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/artificial-intelligence/guidance-on-ai-and-data-protection/how-do-we-ensure-fairness-in-ai/what-about-fairness-bias-and-discrimination/)\n",
    "capabilities--context-length@vvpYkmycH0_W030E-L12f": "# Capabilities / Context Length\n\nA key aspect of the OpenAI models is their context length, which refers to the amount of input text the model can process at once. Earlier models like GPT-3 had a context length of up to 4,096 tokens (words or word pieces), while more recent models like GPT-4 can handle significantly larger context lengths, some supporting up to 32,768 tokens. This extended context length enables the models to handle more complex tasks, such as maintaining long conversations or processing lengthy documents, which enhances their utility in real-world applications like legal document analysis or code generation.\n\nLearn more from the following resources:\n\n- [@official@Managing Context](https://platform.openai.com/docs/guides/conversation-state?api-mode=responses#managing-context-for-text-generation)\n- [@official@Capabilities](https://platform.openai.com/docs/guides/text-generation)\n",
    "chat-completions-api@_bPTciEA1GT1JwfXim19z": "# Chat Completions API\n\nThe OpenAI Chat Completions API is a powerful interface that allows developers to integrate conversational AI into applications by utilizing models like GPT-3.5 and GPT-4. It is designed to manage multi-turn conversations, keeping context across interactions, making it ideal for chatbots, virtual assistants, and interactive AI systems. With the API, users can structure conversations by providing messages in a specific format, where each message has a role (e.g., \"system\" to guide the model, \"user\" for input, and \"assistant\" for responses).\n\nLearn more from the following resources:\n\n- [@official@Create Chat Completions](https://platform.openai.com/docs/api-reference/chat/create)\n- [@article@Getting Start with Chat Completions API](https://medium.com/the-ai-archives/getting-started-with-openais-chat-completions-api-in-2024-462aae00bf0a)\n",
    "chroma@dSd2C9lNl-ymmCRT9_ZC3": "# Chroma\n\nChroma is an open-source vector database and AI-native embedding database designed to handle and store large-scale embeddings and semantic vectors. It is used in applications that require fast, efficient similarity searches, such as natural language processing (NLP), machine learning (ML), and AI systems dealing with text, images, and other high-dimensional data.\n\nVisit the following resources to learn more:\n\n- [@official@Chroma](https://www.trychroma.com/)\n- [@article@Chroma Tutorials](https://lablab.ai/tech/chroma)\n- [@video@Chroma - Chroma - Vector Database for LLM Applications](https://youtu.be/Qs_y0lTJAp0?si=Z2-eSmhf6PKrEKCW)\n",
    "chunking@mX987wiZF7p3V_gExrPeX": "# Chunking\n\nThe chunking step in Retrieval-Augmented Generation (RAG) involves breaking down large documents or data sources into smaller, manageable chunks. This is done to ensure that the retriever can efficiently search through large volumes of data while staying within the token or input limits of the model. Each chunk, typically a paragraph or section, is converted into an embedding, and these embeddings are stored in a vector database. When a query is made, the retriever searches for the most relevant chunks rather than the entire document, enabling faster and more accurate retrieval.\n\nLearn more from the following resources:\n\n- [@article@Understanding LangChain's RecursiveCharacterTextSplitter](https://dev.to/eteimz/understanding-langchains-recursivecharactertextsplitter-2846)\n- [@article@Chunking Strategies for LLM Applications](https://www.pinecone.io/learn/chunking-strategies/)\n- [@article@A Guide to Chunking Strategies for Retrieval Augmented Generation](https://zilliz.com/learn/guide-to-chunking-strategies-for-rag)",
    "code-completion-tools@TifVhqFm1zXNssA8QR3SM": "# Code Completion Tools\n\nCode completion tools are AI-powered development assistants designed to enhance productivity by automatically suggesting code snippets, functions, and entire blocks of code as developers type. These tools, such as GitHub Copilot and Tabnine, leverage machine learning models trained on vast code repositories to predict and generate contextually relevant code. They help reduce repetitive coding tasks, minimize errors, and accelerate the development process by offering real-time, intelligent suggestions.\n\nLearn more from the following resources:\n\n- [@official@GitHub Copilot](https://github.com/features/copilot)\n- [@official@Codeium](https://codeium.com/)\n- [@official@Supermaven](https://supermaven.com/)\n- [@official@Tabnine](https://www.tabnine.com/)",
    "cohere@a7qsvoauFe5u953I699ps": "# Cohere\n\nCohere is an AI platform that specializes in natural language processing (NLP) by providing large language models designed to help developers build and deploy text-based applications. Cohere’s models are used for tasks such as text classification, language generation, semantic search, and sentiment analysis. Unlike some other providers, Cohere emphasizes simplicity and scalability, offering an easy-to-use API that allows developers to fine-tune models on custom data for specific use cases. Additionally, Cohere provides robust multilingual support and focuses on ensuring that its NLP solutions are both accessible and enterprise-ready, catering to a wide range of industries.\n\nLearn more from the following resources:\n\n- [@official@Cohere](https://cohere.com/)\n- [@article@What Does Cohere Do?](https://medium.com/geekculture/what-does-cohere-do-cdadf6d70435)",
    "conducting-adversarial-testing@Pt-AJmSJrOxKvolb5_HEv": "# Conducting adversarial testing\n\nAdversarial testing involves intentionally exposing machine learning models to deceptive, perturbed, or carefully crafted inputs to evaluate their robustness and identify vulnerabilities. The goal is to simulate potential attacks or edge cases where the model might fail, such as subtle manipulations in images, text, or data that cause the model to misclassify or produce incorrect outputs. This type of testing helps to improve model resilience, particularly in sensitive applications like cybersecurity, autonomous systems, and finance.\n\nLearn more from the following resources:\n\n- [@article@Adversarial Testing for Generative AI](https://developers.google.com/machine-learning/resources/adv-testing)\n- [@article@Adversarial Testing: Definition, Examples and Resources](https://www.leapwork.com/blog/adversarial-testing)",
    "constraining-outputs-and-inputs@ONLDyczNacGVZGojYyJrU": "# Constraining outputs and inputs\n\nConstraining outputs and inputs in AI models refers to implementing limits or rules that guide both the data the model processes (inputs) and the results it generates (outputs). Input constraints ensure that only valid, clean, and well-formed data enters the model, which helps to reduce errors and improve performance. This can include setting data type restrictions, value ranges, or specific formats. Output constraints, on the other hand, ensure that the model produces appropriate, safe, and relevant results, often by limiting output length, specifying answer formats, or applying filters to avoid harmful or biased responses. These constraints are crucial for improving model safety, alignment, and utility in practical applications.\n\nLearn more from the following resources:\n\n- [@article@Preventing Prompt Injection](https://learnprompting.org/docs/prompt_hacking/defensive_measures/introduction)\n- [@article@Introducing Structured Outputs in the API - OpenAI](https://openai.com/index/introducing-structured-outputs-in-the-api/)",
    "cut-off-dates--knowledge@LbB2PeytxRSuU07Bk0KlJ": "# Cut-off Dates / Knowledge\n\nOpenAI models, such as GPT-3.5 and GPT-4, have a knowledge cutoff date, which refers to the last point in time when the model was trained on data. For instance, as of the current version of GPT-4, the knowledge cutoff is October 2023. This means the model does not have awareness or knowledge of events, advancements, or data that occurred after that date. Consequently, the model may lack information on more recent developments, research, or real-time events unless explicitly updated in future versions. This limitation is important to consider when using the models for time-sensitive tasks or inquiries involving recent knowledge.\n\nLearn more from the following resources:\n\n- [@article@Knowledge Cutoff Dates of all LLMs explained](https://otterly.ai/blog/knowledge-cutoff/)\n- [@article@Knowledge Cutoff Dates For ChatGPT, Meta Ai, Copilot, Gemini, Claude](https://computercity.com/artificial-intelligence/knowledge-cutoff-dates-llms)",
    "dall-e-api@LKFwwjtcawJ4Z12X102Cb": "# DALL-E API\n\nThe DALL-E API is a tool provided by OpenAI that allows developers to integrate the DALL-E image generation model into applications. DALL-E is an AI model designed to generate images from textual descriptions, capable of producing highly detailed and creative visuals. The API enables users to provide a descriptive prompt, and the model generates corresponding images, opening up possibilities in fields like design, advertising, content creation, and art.\n\nLearn more from the following resources:\n\n- [@official@OpenAI Image Generation](https://platform.openai.com/docs/guides/images)\n- [@video@DALL E API - Introduction (Generative AI Pictures from OpenAI)](https://www.youtube.com/watch?v=Zr6vAWwjHN0)",
    "data-classification@06Xta-OqSci05nV2QMFdF": "# Data Classification\n\nOnce data is embedded, a classification algorithm, such as a neural network or a logistic regression model, can be trained on these embeddings to classify the data into different categories. The advantage of using embeddings is that they capture underlying relationships and similarities between data points, even if the raw data is complex or high-dimensional, improving classification accuracy in tasks like text classification, image categorization, and recommendation systems.\n\nLearn more from the following resources:\n\n- [@article@What Is Data Classification?](https://www.paloaltonetworks.com/cyberpedia/data-classification)\n- [@video@Text Embeddings, Classification, and Semantic Search (w/ Python Code)](https://www.youtube.com/watch?v=sNa_uiqSlJo)",
    "development-tools@NYge7PNtfI-y6QWefXJ4d": "# Development Tools\n\nAI has given rise to a collection of AI powered development tools of various different varieties. We have IDEs like Cursor that has AI baked into it, live context capturing tools such as Pieces and a variety of brower based tools like V0, Claude and more.\n\nLearn more from the following resources:\n\n- [@official@v0 Website](https://v0.dev)\n- [@official@Aider - AI Pair Programming in Terminal](https://aider.chat/)\n- [@official@Replit AI](https://replit.com/ai)\n- [@official@Pieces](https://pieces.app)\n",
    "embedding@grTcbzT7jKk_sIUwOTZTD": "# Embedding\n\nIn Retrieval-Augmented Generation (RAG), embeddings are essential for linking information retrieval with natural language generation. Embeddings represent both the user query and documents as dense vectors in a shared space, enabling the system to retrieve relevant information based on similarity. This retrieved information is then fed into a generative model, such as GPT, to produce contextually informed and accurate responses. By using embeddings, RAG enhances the model's ability to generate content grounded in external knowledge, making it effective for tasks like question answering and summarization.\n\nLearn more from the following resources:\n\n- [@article@Understanding the role of embeddings in RAG LLMs](https://www.aporia.com/learn/understanding-the-role-of-embeddings-in-rag-llms/)\n- [@article@Mastering RAG: How to Select an Embedding Model](https://www.rungalileo.io/blog/mastering-rag-how-to-select-an-embedding-model)\n",
    "embeddings@XyEp6jnBSpCxMGwALnYfT": "# Embeddings\n\nEmbeddings are dense, continuous vector representations of data, such as words, sentences, or images, in a lower-dimensional space. They capture the semantic relationships and patterns in the data, where similar items are placed closer together in the vector space. In machine learning, embeddings are used to convert complex data into numerical form that models can process more easily. For example, word embeddings represent words based on their meanings and contexts, allowing models to understand relationships like synonyms or analogies. Embeddings are widely used in tasks like natural language processing, recommendation systems, and image recognition to improve model performance and efficiency.\n\nLearn more from the following resources:\n\n- [@article@What are Embeddings in Machine Learning?](https://www.cloudflare.com/en-gb/learning/ai/what-are-embeddings/)\n- [@article@What is Embedding?](https://www.ibm.com/topics/embedding)\n- [@video@What are Word Embeddings](https://www.youtube.com/watch?v=wgfSDrqYMJ4)\n",
    "faiss@JurLbOO1Z8r6C3yUqRNwf": "# FAISS\n\nFAISS (Facebook AI Similarity Search) is a library developed by Facebook AI for efficient similarity search and clustering of dense vectors, particularly useful for large-scale datasets. It is optimized to handle embeddings (vector representations) and enables fast nearest neighbor search, allowing you to retrieve similar items from a large collection of vectors based on distance or similarity metrics like cosine similarity or Euclidean distance. FAISS is widely used in applications such as image and text retrieval, recommendation systems, and large-scale search systems where embeddings are used to represent items. It offers several indexing methods and can scale to billions of vectors, making it a powerful tool for handling real-time, large-scale similarity search problems efficiently.\n\nLearn more from the following resources:\n\n- [@official@FAISS](https://ai.meta.com/tools/faiss/)\n- [@video@FAISS Vector Library with LangChain and OpenAI](https://www.youtube.com/watch?v=ZCSsIkyCZk4)\n- [@article@What Is Faiss (Facebook AI Similarity Search)?](https://www.datacamp.com/blog/faiss-facebook-ai-similarity-search)",
    "fine-tuning@15XOFdVp0IC-kLYPXUJWh": "# Fine-tuning\n\nFine-tuning the OpenAI API involves adapting pre-trained models, such as GPT, to specific use cases by training them on custom datasets. This process allows you to refine the model's behavior and improve its performance on specialized tasks, like generating domain-specific text or following particular patterns. By providing labeled examples of the desired input-output pairs, you guide the model to better understand and predict the appropriate responses for your use case.\n\nLearn more from the following resources:\n\n- [@official@Fine-tuning Documentation](https://platform.openai.com/docs/guides/fine-tuning)\n- [@video@Fine-tuning ChatGPT with OpenAI Tutorial](https://www.youtube.com/watch?v=VVKcSf6r3CM)",
    "generation@2jJnS9vRYhaS69d6OxrMh": "# Generation\n\nGeneration refers to the process where a generative language model, such as GPT, creates a response based on the information retrieved during the retrieval phase. After relevant documents or data snippets are identified using embeddings, they are passed to the generative model, which uses this information to produce coherent, context-aware, and informative responses. The retrieved content helps the model stay grounded and factual, enhancing its ability to answer questions, provide summaries, or engage in dialogue by combining retrieved knowledge with its natural language generation capabilities. This synergy between retrieval and generation makes RAG systems effective for tasks that require detailed, accurate, and contextually relevant outputs.\n\nLearn more from the following resources:\n\n- [@article@What is RAG (Retrieval-Augmented Generation)?](https://aws.amazon.com/what-is/retrieval-augmented-generation/)\n- [@video@Retrieval Augmented Generation (RAG) Explained in 8 Minutes!](https://www.youtube.com/watch?v=HREbdmOSQ18)",
    "googles-gemini@oe8E6ZIQWuYvHVbYJHUc1": "# Google's Gemini\n\nGoogle Gemini is an advanced AI model by Google DeepMind, designed to integrate natural language processing with multimodal capabilities, enabling it to understand and generate not just text but also images, videos, and other data types. It combines generative AI with reasoning skills, making it effective for complex tasks requiring logical analysis and contextual understanding. Built on Google's extensive knowledge base and infrastructure, Gemini aims to offer high accuracy, efficiency, and safety, positioning it as a competitor to models like OpenAI's GPT-4.\n\nLearn more from the following resources:\n\n- [@official@Google Gemini](https://gemini.google.com/)\n- [@official@Google's Gemini Documentation](https://workspace.google.com/solutions/ai/)\n- [@video@Welcome to the Gemini era](https://www.youtube.com/watch?v=_fuimO6ErKI)",
    "hugging-face-hub@YLOdOvLXa5Fa7_mmuvKEi": "# Hugging Face Hub\n\nThe Hugging Face Hub is a comprehensive platform that hosts over 900,000 machine learning models, 200,000 datasets, and 300,000 demo applications, facilitating collaboration and sharing within the AI community. It serves as a central repository where users can discover, upload, and experiment with various models and datasets across multiple domains, including natural language processing, computer vision, and audio tasks. It also supports version control.\n\nLearn more from the following resources:\n\n- [@official@Hugging Face Documentation](https://huggingface.co/docs/hub/en/index)\n- [@course@nlp-official](https://huggingface.co/learn/nlp-course/en/chapter4/1)\n",
    "hugging-face-models@8XjkRqHOdyH-DbXHYiBEt": "# Hugging Face Models\n\nHugging Face models are a collection of pre-trained machine learning models available through the Hugging Face platform, covering a wide range of tasks like natural language processing, computer vision, and audio processing. The platform includes models for tasks such as text classification, translation, summarization, question answering, and more, with popular models like BERT, GPT, T5, and CLIP. Hugging Face provides easy-to-use tools and APIs that allow developers to access, fine-tune, and deploy these models, fostering a collaborative community where users can share, modify, and contribute models to improve AI research and application development.\n\nLearn more from the following resources:\n\n- [@official@Hugging Face Models](https://huggingface.co/models)",
    "hugging-face-models@EIDbwbdolR_qsNKVDla6V": "# Hugging Face Models\n\nHugging Face models are a collection of pre-trained machine learning models available through the Hugging Face platform, covering a wide range of tasks like natural language processing, computer vision, and audio processing. The platform includes models for tasks such as text classification, translation, summarization, question answering, and more, with popular models like BERT, GPT, T5, and CLIP. Hugging Face provides easy-to-use tools and APIs that allow developers to access, fine-tune, and deploy these models, fostering a collaborative community where users can share, modify, and contribute models to improve AI research and application development.\n\nLearn more from the following resources:\n\n- [@official@Hugging Face Models](https://huggingface.co/models)\n- [@video@How to Use Pretrained Models from Hugging Face in a Few Lines of Code](https://www.youtube.com/watch?v=ntz160EnWIc)",
    "hugging-face-tasks@YKIPOiSj_FNtg0h8uaSMq": "# Hugging Face Tasks\n\nHugging Face supports text classification, named entity recognition, question answering, summarization, and translation. It also extends to multimodal tasks that involve both text and images, such as visual question answering (VQA) and image-text matching. Each task is done by various pre-trained models that can be easily accessed and fine-tuned through the Hugging Face library.\n\nLearn more from the following resources:\n\n- [@official@Task and Model](https://huggingface.co/learn/computer-vision-course/en/unit4/multimodal-models/tasks-models-part1)\n- [@official@Task Summary](https://huggingface.co/docs/transformers/v4.14.1/en/task_summary)\n- [@official@Task Manager](https://huggingface.co/docs/optimum/en/exporters/task_manager)\n",
    "hugging-face@v99C5Bml2a6148LCJ9gy9": "# Hugging Face\n\nHugging Face is a leading AI company and open-source platform that provides tools, models, and libraries for natural language processing (NLP), computer vision, and other machine learning tasks. It is best known for its \"Transformers\" library, which simplifies the use of pre-trained models like BERT, GPT, T5, and CLIP, making them accessible for tasks such as text classification, translation, summarization, and image recognition.\n\nLearn more from the following resources:\n\n- [@official@Hugging Face](https://huggingface.co)\n- [@video@What is Hugging Face? - Machine Learning Hub Explained](https://www.youtube.com/watch?v=1AUjKfpRZVo)\n- [@course@Hugging Face Official Video Course](https://www.youtube.com/watch?v=00GKzGyWFEs&list=PLo2EIpI_JMQvWfQndUesu0nPBAtZ9gP1o)\n",
    "image-generation@49BWxYVFpIgZCCqsikH7l": "# Image Generation\n\nImage generation is a process in artificial intelligence where models create new images based on input prompts or existing data. It involves using generative models like GANs (Generative Adversarial Networks), VAEs (Variational Autoencoders), or more recently, transformer-based models like DALL-E and Stable Diffusion.\n\nLearn more from the following resources:\n\n- [@official@DALL-E](https://openai.com/index/dall-e-2/)\n- [@article@How DALL-E 2 Actually Works](https://www.assemblyai.com/blog/how-dall-e-2-actually-works/)\n- [@video@How AI Image Generators Work (Stable Diffusion / Dall-E)](https://www.youtube.com/watch?v=1CIpzeNxIhU)",
    "image-understanding@fzVq4hGoa2gdbIzoyY1Zp": "# Image Understanding\n\nMultimodal AI enhances image understanding by integrating visual data with other types of information, such as text or audio. By combining these inputs, AI models can interpret images more comprehensively, recognizing objects, scenes, and actions, while also understanding context and related concepts. For example, an AI system could analyze an image and generate descriptive captions, or provide explanations based on both visual content and accompanying text.\n\nLearn more from the following resources:\n\n- [@article@Low or High Fidelity Image Understanding - OpenAI](https://platform.openai.com/docs/guides/images)\n",
    "impact-on-product-development@qJVgKe9uBvXc-YPfvX_Y7": "# Impact on Product Development\n\nAI engineering transforms product development by automating tasks, enhancing data-driven decision-making, and enabling the creation of smarter, more personalized products. It speeds up design cycles, optimizes processes, and allows for predictive maintenance, quality control, and efficient resource management. By integrating AI, companies can innovate faster, reduce costs, and improve user experiences, giving them a competitive edge in the market.\n\nLearn more from the following resources:\n\n- [@article@AI in Product Development: Netflix, BMW, and PepsiCo](https://www.virtasant.com/ai-today/ai-in-product-development-netflix-bmw#:~:text=AI%20can%20help%20make%20product,and%20gain%20a%20competitive%20edge.)\n- [@article@AI Product Development: Why Are Founders So Fascinated By The Potential?](https://www.techmagic.co/blog/ai-product-development/)\n",
    "indexing-embeddings@5TQnO9B4_LTHwqjI7iHB1": "# Indexing Embeddings\n\nEmbeddings are stored in a vector database by first converting data, such as text, images, or audio, into high-dimensional vectors using machine learning models. These vectors, also called embeddings, capture the semantic relationships and patterns within the data. Once generated, each embedding is indexed in the vector database along with its associated metadata, such as the original data (e.g., text or image) or an identifier. The vector database then organizes these embeddings to support efficient similarity searches, typically using techniques like approximate nearest neighbor (ANN) search.\n\nLearn more from the following resources:\n\n- [@article@Indexing & Embeddings](https://docs.llamaindex.ai/en/stable/understanding/indexing/indexing/)\n- [@video@Vector Databases Simply Explained! (Embeddings & Indexes)](https://www.youtube.com/watch?v=dN0lsF2cvm4)",
    "inference-sdk@3kRTzlLNBnXdTsAEXVu_M": "# Inference SDK\n\nThe Hugging Face Inference SDK is a powerful tool that allows developers to easily integrate and run inference on large language models hosted on the Hugging Face Hub. By using the `InferenceClient`, users can make API calls to various models for tasks such as text generation, image creation, and more. The SDK supports both synchronous and asynchronous operations thus compatible with existing workflows.\n\nLearn more from the following resources:\n\n- [@official@Inference](https://huggingface.co/docs/huggingface_hub/en/package_reference/inference_client)\n- [@article@Endpoint Setup](https://www.npmjs.com/package/@huggingface/inference)\n",
    "inference@KWjD4xEPhOOYS51dvRLd2": "# Inference\n\nIn artificial intelligence (AI), inference refers to the process where a trained machine learning model makes predictions or draws conclusions from new, unseen data. Unlike training, inference involves the model applying what it has learned to make decisions without needing examples of the exact result. In essence, inference is the AI model actively functioning. For example, a self-driving car recognizing a stop sign on a road it has never encountered before demonstrates inference. The model identifies the stop sign in a new setting, using its learned knowledge to make a decision in real-time.\n\nLearn more from the following resources:\n\n- [@article@Inference vs Training](https://www.cloudflare.com/learning/ai/inference-vs-training/)\n- [@article@What is Machine Learning Inference?](https://hazelcast.com/glossary/machine-learning-inference/)\n- [@article@What is Machine Learning Inference? An Introduction to Inference Approaches](https://www.datacamp.com/blog/what-is-machine-learning-inference)",
    "introduction@_hYN0gEi9BL24nptEtXWU": "# Introduction\n\nAI Engineering is the process of designing and implementing AI systems using pre-trained models and existing AI tools to solve practical problems. AI Engineers focus on applying AI in real-world scenarios, improving user experiences, and automating tasks, without developing new models from scratch. They work to ensure AI systems are efficient, scalable, and can be seamlessly integrated into business applications, distinguishing their role from AI Researchers and ML Engineers, who concentrate more on creating new models or advancing AI theory.\n\nLearn more from the following resources:\n\n- [@video@AI vs Machine Learning](https://www.youtube.com/watch?v=4RixMPF4xis)\n- [@video@AI vs Machine Learning vs Deep Learning vs GenAI](https://youtu.be/qYNweeDHiyU?si=eRJXjtk8Q-RKQ8Ms)\n- [@article@AI Engineering](https://en.wikipedia.org/wiki/Artificial_intelligence_engineering)\n",
    "know-your-customers--usecases@t1SObMWkDZ1cKqNNlcd9L": "# Know your Customers / Usecases\n\nTo know your customer means deeply understanding the needs, behaviors, and expectations of your target users. This ensures the tools you create are tailored precisely for their intended purpose, while also being designed to prevent misuse or unintended applications. By clearly defining the tool’s functionality and boundaries, you can align its features with the users’ goals while incorporating safeguards that limit its use in contexts it wasn’t designed for. This approach enhances both the tool’s effectiveness and safety, reducing the risk of improper use.\n\nLearn more from the following resources:\n\n- [@article@Assigning Roles](https://learnprompting.org/docs/basics/roles)\n",
    "lancedb@rjaCNT3Li45kwu2gXckke": "# LanceDB\n\nLanceDB is a vector database designed for efficient storage, retrieval, and management of embeddings. It enables users to perform fast similarity searches, particularly useful in applications like recommendation systems, semantic search, and AI-driven content retrieval. LanceDB focuses on scalability and speed, allowing large-scale datasets of embeddings to be indexed and queried quickly, which is essential for real-time AI applications. It integrates well with machine learning workflows, making it easier to deploy models that rely on vector-based data processing, and helps manage the complexities of handling high-dimensional vector data efficiently.\n\nLearn more from the following resources:\n\n- [@official@LanceDB](https://lancedb.com/)\n- [@official@LanceDB Documentation](https://docs.lancedb.com/enterprise/introduction)\n- [@opensource@LanceDB on GitHub](https://github.com/lancedb/lancedb)\n",
    "langchain-for-multimodal-apps@j9zD3pHysB1CBhLfLjhpD": "# LangChain for Multimodal Apps\n\nLangChain is a framework designed to build applications that integrate multiple AI models, especially those focusing on language understanding, generation, and multimodal capabilities. For multimodal apps, LangChain facilitates seamless interaction between text, image, and even audio models, enabling developers to create complex workflows that can process and analyze different types of data.\n\nLearn more from the following resources:\n\n- [@official@LangChain](https://www.langchain.com/)\n- [@video@Build a Multimodal GenAI App with LangChain and Gemini LLMs](https://www.youtube.com/watch?v=bToMzuiOMhg)",
    "langchain@ebXXEhNRROjbbof-Gym4p": "# Langchain\n\nLangChain is a development framework that simplifies building applications powered by language models, enabling seamless integration of multiple AI models and data sources. It focuses on creating chains, or sequences, of operations where language models can interact with databases, APIs, and other models to perform complex tasks. LangChain offers tools for prompt management, data retrieval, and workflow orchestration, making it easier to develop robust, scalable applications like chatbots, automated data analysis, and multi-step reasoning systems.\n\nLearn more from the following resources:\n\n- [@official@LangChain](https://www.langchain.com/)\n- [@video@What is LangChain?](https://www.youtube.com/watch?v=1bUy-1hGZpI)\n",
    "limitations-and-considerations@MXqbQGhNM3xpXlMC2ib_6": "# Limitations and Considerations\n\nPre-trained models, while powerful, come with several limitations and considerations. They may carry biases present in the training data, leading to unintended or discriminatory outcomes, these models are also typically trained on general data, so they might not perform well on niche or domain-specific tasks without further fine-tuning. Another concern is the \"black-box\" nature of many pre-trained models, which can make their decision-making processes hard to interpret and explain.\n\nLearn more from the following resources:\n\n- [@article@Pre-trained Topic Models: Advantages and Limitation](https://www.kaggle.com/code/amalsalilan/pretrained-topic-models-advantages-and-limitation)\n- [@video@Should You Use Open Source Large Language Models?](https://www.youtube.com/watch?v=y9k-U9AuDeM)",
    "llama-index@d0ontCII8KI8wfP-8Y45R": "# LlamaIndex\n\nLlamaIndex, formerly known as GPT Index, is a tool designed to facilitate the integration of large language models (LLMs) with structured and unstructured data sources. It acts as a data framework that helps developers build retrieval-augmented generation (RAG) applications by indexing various types of data, such as documents, databases, and APIs, enabling LLMs to query and retrieve relevant information efficiently.\n\nLearn more from the following resources:\n\n- [@official@Llama Index](https://docs.llamaindex.ai/en/stable/)\n- [@video@Introduction to LlamaIndex with Python (2024)](https://www.youtube.com/watch?v=cCyYGYyCka4)",
    "llamaindex-for-multimodal-apps@akQTCKuPRRelj2GORqvsh": "# LlamaIndex for Multi-modal Apps\n\nLlamaIndex enables multi-modal apps by linking language models (LLMs) to diverse data sources, including text and images. It indexes and retrieves information across formats, allowing LLMs to process and integrate data from multiple modalities. This supports applications like visual question answering, content summarization, and interactive systems by providing structured, context-aware inputs from various content types.\n\nLearn more from the following resources:\n\n- [@official@LlamaIndex Multi-modal](https://docs.llamaindex.ai/en/stable/use_cases/multimodal/)\n- [@video@Multi-modal Retrieval Augmented Generation with LlamaIndex](https://www.youtube.com/watch?v=35RlrrgYDyU)",
    "llms@wf2BSyUekr1S1q6l8kyq6": "# LLMs\n\nLLMs, or Large Language Models, are advanced AI models trained on vast datasets to understand and generate human-like text. They can perform a wide range of natural language processing tasks, such as text generation, translation, summarization, and question answering. Examples include GPT-4, BERT, and T5. LLMs are capable of understanding context, handling complex queries, and generating coherent responses, making them useful for applications like chatbots, content creation, and automated support. However, they require significant computational resources and may carry biases from their training data.\n\nLearn more from the following resources:\n\n- [@article@What is a large language model (LLM)?](https://www.cloudflare.com/en-gb/learning/ai/what-is-large-language-model/)\n- [@video@How Large Language Models Work](https://www.youtube.com/watch?v=5sLYAQS9sWQ)\n- [@video@Large Language Models (LLMs) - Everything You NEED To Know](https://www.youtube.com/watch?v=osKyvYJ3PRM)\n",
    "manual-implementation@6xaRB34_g0HGt-y1dGYXR": "# Manual Implementation\n\nServices like Open AI functions and Tools or Vercel's AI SDK make it really easy to make SDK agents however it is a good idea to learn how these tools work under the hood. You can also create fully custom implementation of agents using by implementing custom loop.\n\nLearn more from the following resources:\n\n- [@official@OpenAI Function Calling](https://platform.openai.com/docs/guides/function-calling)\n- [@official@Vercel AI SDK](https://sdk.vercel.ai/docs/foundations/tools)",
    "maximum-tokens@qzvp6YxWDiGakA2mtspfh": "# Maximum Tokens\n\nThe OpenAI API has different maximum token limits depending on the model being used. For instance, GPT-3 has a limit of 4,096 tokens, while GPT-4 can support larger inputs, with some versions allowing up to 8,192 tokens, and extended versions reaching up to 32,768 tokens. Tokens include both the input text and the generated output, so longer inputs mean less space for responses. Managing token limits is crucial to ensure the model can handle the entire input and still generate a complete response, especially for tasks involving lengthy documents or multi-turn conversations.\n\nLearn more from the following resources:\n\n- [@official@Maximum Tokens](https://platform.openai.com/docs/guides/rate-limits)\n- [@article@The Ins and Outs of GPT Token Limits](https://www.supernormal.com/blog/gpt-token-limits)",
    "mistral-ai@n-Ud2dXkqIzK37jlKItN4": "# Mistral AI\n\nMistral AI is a company focused on developing open-weight, large language models (LLMs) to provide high-performance AI solutions. Mistral aims to create models that are both efficient and versatile, making them suitable for a wide range of natural language processing tasks, including text generation, translation, and summarization. By releasing open-weight models, Mistral promotes transparency and accessibility, allowing developers to customize and deploy AI solutions more flexibly compared to proprietary models.\n\nLearn more from the resources:\n\n- [@official@Mistral AI](https://mistral.ai/)\n- [@video@Mistral AI: The Gen AI Start-up you did not know existed](https://www.youtube.com/watch?v=vzrRGd18tAg)",
    "models-on-hugging-face@dLEg4IA3F5jgc44Bst9if": "# Models on Hugging Face\n\nEmbedding models are used to convert raw data like text, code, or images into high-dimensional vectors that capture semantic meaning. These vector representations allow AI systems to compare, cluster, and retrieve information based on similarity rather than exact matches. Hugging Face provides a wide range of pretrained embedding models such as `all-MiniLM-L6-v2`, `gte-base`, `Qwen3-Embedding-8B` and `bge-base` which are commonly used for tasks like semantic search, recommendation systems, duplicate detection, and retrieval-augmented generation (RAG). These models can be accessed through libraries like transformers or sentence-transformers, making it easy to generate high-quality embeddings for both general-purpose and task-specific applications.\n\nLearn more from the following resources:\n- [@video@Hugging Face - Text embeddings & semantic search](https://www.youtube.com/watch?v=OATCgQtNX2o)\n- [@official@Hugging Face Embedding Models](https://huggingface.co/models?pipeline_tag=feature-extraction)\n",
    "mongodb-atlas@j6bkm0VUgLkHdMDDJFiMC": "# MongoDB Atlas\n\nMongoDB Atlas, traditionally known for its document database capabilities, now includes vector search functionality, making it a strong option as a vector database. This feature allows developers to store and query high-dimensional vector data alongside regular document data. With Atlas’s vector search, users can perform similarity searches on embeddings of text, images, or other complex data, making it ideal for AI and machine learning applications like recommendation systems, image similarity search, and natural language processing tasks. The seamless integration of vector search within the MongoDB ecosystem allows developers to leverage familiar tools and interfaces while benefiting from advanced vector-based operations for sophisticated data analysis and retrieval.\n\nLearn more from the following resources:\n\n- [@official@Vector Search in MongoDB Atlas](https://www.mongodb.com/products/platform/atlas-vector-search)",
    "multimodal-ai-usecases@sGR9qcro68KrzM8qWxcH8": "# Multimodal AI Usecases\n\nMultimodal AI powers applications like visual question answering, content moderation, and enhanced search engines. It drives smarter virtual assistants and interactive AR apps, combining text, images, and audio for richer, more intuitive user experiences across e-commerce, accessibility, and entertainment.\n\nLearn more from the following resources:\n\n- [@official@Hugging Face Multimodal Models](https://huggingface.co/learn/computer-vision-course/en/unit4/multimodal-models/a_multimodal_world)",
    "multimodal-ai@W7cKPt_UxcUgwp8J6hS4p": "# Multimodal AI\n\nMultimodal AI is an approach that combines and processes data from multiple sources, such as text, images, audio, and video, to understand and generate responses. By integrating different data types, it enables more comprehensive and accurate AI systems, allowing for tasks like visual question answering, interactive virtual assistants, and enhanced content understanding. This capability helps create richer, more context-aware applications that can analyze and respond to complex, real-world scenarios.\n\nLearn more from the following resources:\n\n- [@article@A Multimodal World - Hugging Face](https://huggingface.co/learn/computer-vision-course/en/unit4/multimodal-models/a_multimodal_world)\n- [@article@Multimodal AI - Google](https://cloud.google.com/use-cases/multimodal-ai?hl=en)\n- [@article@What Is Multimodal AI? A Complete Introduction](https://www.splunk.com/en_us/blog/learn/multimodal-ai.html)",
    "ollama-models@ro3vY_sp6xMQ-hfzO-rc1": "# Ollama Models\n\nOllama provides a collection of large language models (LLMs) designed to run locally on personal devices, enabling privacy-focused and efficient AI applications without relying on cloud services. These models can perform tasks like text generation, translation, summarization, and question answering, similar to popular models like GPT. Ollama emphasizes ease of use, offering models that are optimized for lower resource consumption, making it possible to deploy AI capabilities directly on laptops or edge devices.\n\nLearn more from the following resources:\n\n- [@official@Ollama Model Library](https://ollama.com/library)\n- [@video@What are the different types of models? Ollama Course](https://www.youtube.com/watch?v=f4tXwCNP1Ac)\n",
    "ollama-sdk@TsG_I7FL-cOCSw8gvZH3r": "# Ollama SDK\n\nThe Ollama SDK is a community-driven tool that allows developers to integrate and run large language models (LLMs) locally through a simple API. Enabling users to easily import the Ollama provider and create customized instances for various models, such as Llama 2 and Mistral. The SDK supports functionalities like `text generation` and `embeddings`, making it versatile for applications ranging from `chatbots` to `content generation`. Also Ollama SDK enhances privacy and control over data while offering seamless integration with existing workflows.\n\nLearn more from the following resources:\n\n- [@article@SDK Provider](https://sdk.vercel.ai/providers/community-providers/ollama)\n- [@article@Beginner's Guide](https://dev.to/jayantaadhikary/using-the-ollama-api-to-run-llms-and-generate-responses-locally-18b7)\n- [@article@Setup](https://klu.ai/glossary/ollama)\n",
    "ollama@rTT2UnvqFO3GH6ThPLEjO": "# Ollama\n\nOllama is a platform that offers large language models (LLMs) designed to run locally on personal devices, enabling AI functionality without relying on cloud services. It focuses on privacy, performance, and ease of use by allowing users to deploy models directly on laptops, desktops, or edge devices, providing fast, offline AI capabilities. With tools like the Ollama SDK, developers can integrate these models into their applications for tasks such as text generation, summarization, and more, benefiting from reduced latency, greater data control, and seamless local processing.\n\nLearn more from the following resources:\n\n- [@official@Ollama](https://ollama.com/)\n- [@article@Ollama: Easily run LLMs locally](https://klu.ai/glossary/ollama)\n- [@video@What is Ollama? Running Local LLMs Made Simple](https://www.youtube.com/watch?v=5RIOQuHOihY)\n",
    "open-ai-assistant-api@eOqCBgBTKM8CmY3nsWjre": "# OpenAI Assistant API\n\nThe OpenAI Assistant API enables developers to create advanced conversational systems using models like GPT-4. It supports multi-turn conversations, allowing the AI to maintain context across exchanges, which is ideal for chatbots, virtual assistants, and interactive applications. Developers can customize interactions by defining roles, such as system, user, and assistant, to guide the assistant's behavior. With features like temperature control, token limits, and stop sequences, the API offers flexibility to ensure responses are relevant, safe, and tailored to specific use cases.\n\nLearn more from the following resources:\n\n- [@official@Assistants API](https://platform.openai.com/docs/assistants/overview)\n- [@course@OpenAI Assistants API – Course for Beginners](https://www.youtube.com/watch?v=qHPonmSX4Ms)",
    "open-ai-embedding-models@y0qD5Kb4Pf-ymIwW-tvhX": "# OpenAI Embedding Models\n\nOpenAI's embedding models convert text into dense vector representations that capture semantic meaning, allowing for efficient similarity searches, clustering, and recommendations. These models are commonly used for tasks like semantic search, where similar phrases are mapped to nearby points in a vector space, and for building recommendation systems by comparing embeddings to find related content. OpenAI's embedding models offer versatility, supporting a range of applications from document retrieval to content classification, and can be easily integrated through the OpenAI API for scalable and efficient deployment.\n\nLearn more from the following resources:\n\n- [@official@OpenAI Embedding Models](https://platform.openai.com/docs/guides/embeddings/embedding-models)\n- [@video@OpenAI Embeddings Explained in 5 Minutes](https://www.youtube.com/watch?v=8kJStTRuMcs)",
    "open-ai-embeddings-api@l6priWeJhbdUD5tJ7uHyG": "# OpenAI Embeddings API\n\nThe OpenAI Embeddings API allows developers to generate dense vector representations of text, which capture semantic meaning and relationships. These embeddings can be used for various tasks, such as semantic search, recommendation systems, and clustering, by enabling the comparison of text based on similarity in vector space. The API supports easy integration and scalability, making it possible to handle large datasets and perform tasks like finding similar documents, organizing content, or building recommendation engines.\nLearn more from the following resources:\n\n- [@official@OpenAI Embeddings API](https://platform.openai.com/docs/api-reference/embeddings/create)\n- [@video@Master OpenAI Embedding API](https://www.youtube.com/watch?v=9oCS-VQupoc)",
    "open-ai-models@2WbVpRLqwi3Oeqk1JPui4": "# OpenAI Models\n\nOpenAI provides a variety of models designed for diverse tasks. GPT models like GPT-3 and GPT-4 handle text generation, conversation, and translation, offering context-aware responses, while Codex specializes in generating and debugging code across multiple languages. DALL-E creates images from text descriptions, supporting applications in design and content creation, and Whisper is a speech recognition model that converts spoken language to text for transcription and voice-to-text tasks.\n\nLearn more from the following resources:\n\n- [@official@OpenAI Models Overview](https://platform.openai.com/docs/models)\n- [@video@OpenAI’s new “deep-thinking” o1 model crushes coding benchmarks](https://www.youtube.com/watch?v=6xlPJiNpCVw)",
    "open-ai-playground@nyBgEHvUhwF-NANMwkRJW": "# OpenAI Playground\n\nThe OpenAI Playground is an interactive web interface that allows users to experiment with OpenAI's language models, such as GPT-3 and GPT-4, without needing to write code. It provides a user-friendly environment where you can input prompts, adjust parameters like temperature and token limits, and see how the models generate responses in real-time. The Playground helps users test different use cases, from text generation to question answering, and refine prompts for better outputs. It's a valuable tool for exploring the capabilities of OpenAI models, prototyping ideas, and understanding how the models behave before integrating them into applications.\n\nLearn more from the following resources:\n\n- [@official@OpenAI Playground](https://platform.openai.com/playground/chat)\n- [@video@How to Use OpenAi Playground Like a Pro](https://www.youtube.com/watch?v=PLxpvtODiqs)",
    "open-source-embeddings@apVYIV4EyejPft25oAvdI": "# Open-Source Embeddings\n\nOpen-source embeddings are pre-trained vector representations of data, usually text, that are freely available for use and modification. These embeddings capture semantic meanings, making them useful for tasks like semantic search, text classification, and clustering. Examples include Word2Vec, GloVe, and FastText, which represent words as vectors based on their context in large corpora, and more advanced models like Sentence-BERT and CLIP that provide embeddings for sentences and images. Open-source embeddings allow developers to leverage pre-trained models without starting from scratch, enabling faster development and experimentation in natural language processing and other AI applications.\n\nLearn more from the following resources:\n\n- [@official@Embeddings](https://platform.openai.com/docs/guides/embeddings)\n- [@article@A Guide to Open-Source Embedding Models](https://www.bentoml.com/blog/a-guide-to-open-source-embedding-models)",
    "open-vs-closed-source-models@RBwGsq9DngUsl8PrrCbqx": "# Open vs Closed Source Models\n\nOpen-source models are freely available for customization and collaboration, promoting transparency and flexibility, while closed-source models are proprietary, offering ease of use but limiting modification and transparency.\n\nLearn more from the following resources:\n\n- [@article@OpenAI vs. Open Source LLM](https://ubiops.com/openai-vs-open-source-llm/)\n- [@video@Open-Source vs Closed-Source LLMs](https://www.youtube.com/watch?v=710PDpuLwOc)\n",
    "openai-api@zdeuA4GbdBl2DwKgiOA4G": "# OpenAI API\n\nThe OpenAI API provides access to powerful AI models like GPT, Codex, DALL-E, and Whisper, enabling developers to integrate capabilities such as text generation, code assistance, image creation, and speech recognition into their applications via a simple, scalable interface.\n\nLearn more from the following resources:\n\n- [@official@OpenAI API](https://openai.com/api/)\n",
    "openai-assistant-api@mbp2NoL-VZ5hZIIblNBXt": "# OpenAI Assistant API\n\nThe OpenAI Assistant API enables developers to create advanced conversational systems using models like GPT-4. It supports multi-turn conversations, allowing the AI to maintain context across exchanges, which is ideal for chatbots, virtual assistants, and interactive applications. Developers can customize interactions by defining roles, such as system, user, and assistant, to guide the assistant's behavior. With features like temperature control, token limits, and stop sequences, the API offers flexibility to ensure responses are relevant, safe, and tailored to specific use cases.\n\nLearn more from the following resources:\n\n- [@official@Assistants API](https://platform.openai.com/docs/assistants/overview)\n- [@course@OpenAI Assistants API – Course for Beginners](https://www.youtube.com/watch?v=qHPonmSX4Ms)",
    "openai-functions--tools@Sm0Ne5Nx72hcZCdAcC0C2": "# OpenAI Functions / Tools\n\nOpenAI Functions, also known as tools, enable developers to extend the capabilities of language models by integrating external APIs and functionalities, allowing the models to perform specific actions, fetch real-time data, or interact with other software systems. This feature enhances the model's utility by bridging it with services like web searches, databases, and custom business applications, enabling more dynamic and task-oriented responses.\n\nLearn more from the following resources:\n\n- [@official@Function Calling](https://platform.openai.com/docs/guides/function-calling)\n- [@video@How does OpenAI Function Calling work?](https://www.youtube.com/watch?v=Qor2VZoBib0)",
    "openai-models@5ShWZl1QUqPwO-NRGN85V": "# OpenAI Models\n\nOpenAI provides a variety of models designed for diverse tasks. GPT models like GPT-3 and GPT-4 handle text generation, conversation, and translation, offering context-aware responses, while Codex specializes in generating and debugging code across multiple languages. DALL-E creates images from text descriptions, supporting applications in design and content creation, and Whisper is a speech recognition model that converts spoken language to text for transcription and voice-to-text tasks.\n\nLearn more from the following resources:\n\n- [@official@OpenAI Models Overview](https://platform.openai.com/docs/models)",
    "openai-moderation-api@ljZLa3yjQpegiZWwtnn_q": "# OpenAI Moderation API\n\nThe OpenAI Moderation API helps detect and filter harmful content by analyzing text for issues like hate speech, violence, self-harm, and adult content. It uses machine learning models to identify inappropriate or unsafe language, allowing developers to create safer online environments and maintain community guidelines. The API is designed to be integrated into applications, websites, and platforms, providing real-time content moderation to reduce the spread of harmful or offensive material.\n\nLearn more from the following resources:\n\n- [@official@Moderation](https://platform.openai.com/docs/guides/moderation)\n- [@article@How to user the moderation API](https://cookbook.openai.com/examples/how_to_use_moderation)",
    "openai-vision-api@CRrqa-dBw1LlOwVbrZhjK": "# OpenAI Vision API\n\nThe OpenAI Vision API enables models to analyze and understand images, allowing them to identify objects, recognize text, and interpret visual content. It integrates image processing with natural language capabilities, enabling tasks like visual question answering, image captioning, and extracting information from photos. This API can be used for applications in accessibility, content moderation, and automation, providing a seamless way to combine visual understanding with text-based interactions.\n\nLearn more from the following resources:\n\n- [@official@Vision](https://platform.openai.com/docs/guides/vision)\n- [@video@OpenAI Vision API Crash Course](https://www.youtube.com/watch?v=ZjkS11DSeEk)\n",
    "opensource-ai@a_3SabylVqzzOyw3tZN5f": "# OpenSource AI\n\nOpen-source AI refers to AI models, tools, and frameworks that are freely available for anyone to use, modify, and distribute. Examples include TensorFlow, PyTorch, and models like BERT and Stable Diffusion. Open-source AI fosters transparency, collaboration, and innovation by allowing developers to inspect code, adapt models for specific needs, and contribute improvements. This approach accelerates the development of AI technologies, enabling faster experimentation and reducing dependency on proprietary solutions.\n\nLearn more from the following resources:\n\n- [@article@Open Source AI Is the Path Forward](https://about.fb.com/news/2024/07/open-source-ai-is-the-path-forward/)\n- [@video@Should You Use Open Source Large Language Models?](https://www.youtube.com/watch?v=y9k-U9AuDeM)\n",
    "performing-similarity-search@ZcbRPtgaptqKqWBgRrEBU": "# Performing Similarity Search\n\nIn a similarity search, the process begins by converting the user’s query (such as a piece of text or an image) into an embedding—a vector representation that captures the query’s semantic meaning. This embedding is generated using a pre-trained model, such as BERT for text or a neural network for images. Once the query is converted into a vector, it is compared to the embeddings stored in the vector database.\n\nVisit the following resources to learn more:\n\n- [@article@What is Similarity Search & How Does it work?](https://www.truefoundry.com/blog/similarity-search)",
    "pinecone@_Cf7S1DCvX7p1_3-tP3C3": "# Pinecone\n\nPinecone is a managed vector database designed for efficient similarity search and real-time retrieval of high-dimensional data, such as embeddings. It allows developers to store, index, and query vector representations, making it easy to build applications like recommendation systems, semantic search, and AI-driven content discovery. Pinecone is scalable, handles large datasets, and provides fast, low-latency searches using optimized indexing techniques.\n\nLearn more from the following resources:\n\n- [@official@Pinecone](https://www.pinecone.io)\n- [@article@Everything you need to know about Pinecone](https://www.packtpub.com/article-hub/everything-you-need-to-know-about-pinecone-a-vector-database?srsltid=AfmBOorXsy9WImpULoLjd-42ERvTzj3pQb7C2EFgamWlRobyGJVZKKdz)\n- [@video@Introducing Pinecone Serverless](https://www.youtube.com/watch?v=iCuR6ihHQgc)\n",
    "popular-open-source-models@97eu-XxYUH9pYbD_KjAtA": "# Popular Open Source Models\n\nOpen-source large language models (LLMs) are models whose source code and architecture are publicly available for use, modification, and distribution. They are built using machine learning algorithms that process and generate human-like text, and being open-source, they promote transparency, innovation, and community collaboration in their development and application.\n\nLearn more from the following resources:\n\n- [@article@The Best Large Language Models (LLMs) in 2024](https://zapier.com/blog/best-llm/)\n- [@article@8 Top Open-Source LLMs for 2024 and Their Uses](https://www.datacamp.com/blog/top-open-source-llms)\n",
    "pre-trained-models@d7fzv_ft12EopsQdmEsel": "# Pre-trained Models\n\nPre-trained models are Machine Learning (ML) models that have been previously trained on a large dataset to solve a specific task or set of tasks. These models learn patterns, features, and representations from the training data, which can then be fine-tuned or adapted for other related tasks. Pre-training provides a good starting point, reducing the amount of data and computation required to train a new model from scratch.\n\nVisit the following resources to learn more:\n\n- [@article@Pre-trained Models: Past, Present and Future](https://www.sciencedirect.com/science/article/pii/S2666651021000231)\n",
    "pricing-considerations@4GArjDYipit4SLqKZAWDf": "# Pricing Considerations\n\nThe pricing for the OpenAI Embedding API is based on the number of tokens processed and the specific embedding model used. Costs are determined by the total tokens needed to generate embeddings, so longer texts will result in higher charges. To manage costs, developers can optimize by shortening inputs or batching requests. Additionally, selecting the right embedding model for your performance and budget requirements, along with monitoring token usage, can help control expenses.\n\nVisit the following resources to learn more:\n\n- [@official@OpenAI Pricing](https://openai.com/api/pricing/)",
    "pricing-considerations@DZPM9zjCbYYWBPLmQImxQ": "# Pricing Considerations\n\nWhen using the OpenAI API, pricing considerations depend on factors like the model type, usage volume, and specific features utilized. Different models, such as GPT-3.5, GPT-4, or DALL-E, have varying cost structures based on the complexity of the model and the number of tokens processed (inputs and outputs). For cost efficiency, you should optimize prompt design, monitor usage, and consider rate limits or volume discounts offered by OpenAI for high usage.\n\n- [@official@OpenAI API Pricing](https://openai.com/api/pricing/)",
    "prompt-engineering@Dc15ayFlzqMF24RqIF_-X": "# Prompt Engineering\n\nPrompt engineering is the process of crafting effective inputs (prompts) to guide AI models, like GPT, to generate desired outputs. It involves strategically designing prompts to optimize the model’s performance by providing clear instructions, context, and examples. Effective prompt engineering can improve the quality, relevance, and accuracy of responses, making it essential for applications like chatbots, content generation, and automated support. By refining prompts, developers can better control the model’s behavior, reduce ambiguity, and achieve more consistent results, enhancing the overall effectiveness of AI-driven systems.\n\nLearn more from the following resources:\n\n- [@roadmap@Visit Dedicated Prompt Engineering Roadmap](https://roadmap.sh/prompt-engineering)\n- [@video@What is Prompt Engineering?](https://www.youtube.com/watch?v=nf1e-55KKbg)\n",
    "prompt-injection-attacks@cUyLT6ctYQ1pgmodCKREq": "# Prompt Injection Attacks\n\nPrompt injection attacks are a type of security vulnerability where malicious inputs are crafted to manipulate or exploit AI models, like language models, to produce unintended or harmful outputs. These attacks involve injecting deceptive or adversarial content into the prompt to bypass filters, extract confidential information, or make the model respond in ways it shouldn't. For instance, a prompt injection could trick a model into revealing sensitive data or generating inappropriate responses by altering its expected behavior.\n\nLearn more from the following resources:\n\n- [@article@Prompt Injection in LLMs](https://www.promptingguide.ai/prompts/adversarial-prompting/prompt-injection)\n- [@article@What is a Prompt Injection Attack?](https://www.wiz.io/academy/prompt-injection-attack)",
    "purpose-and-functionality@WcjX6p-V-Rdd77EL8Ega9": "# Purpose and Functionality\n\nA vector database is designed to store, manage, and retrieve high-dimensional vectors (embeddings) generated by AI models. Its primary purpose is to perform fast and efficient similarity searches, enabling applications to find data points that are semantically or visually similar to a given query. Unlike traditional databases, which handle structured data, vector databases excel at managing unstructured data like text, images, and audio by converting them into dense vector representations. They use indexing techniques, such as approximate nearest neighbor (ANN) algorithms, to quickly search large datasets and return relevant results. Vector databases are essential for applications like recommendation systems, semantic search, and content discovery, where understanding and retrieving similar items is crucial.\n\nLearn more from the following resources:\n\n- [@article@What is a Vector Database? Top 12 Use Cases](https://lakefs.io/blog/what-is-vector-databases/)\n- [@article@Vector Databases: Intro, Use Cases](https://www.v7labs.com/blog/vector-databases)",
    "qdrant@DwOAL5mOBgBiw-EQpAzQl": "# Qdrant\n\nQdrant is an open-source vector database designed for efficient similarity search and real-time data retrieval. It specializes in storing and indexing high-dimensional vectors (embeddings) to enable fast and accurate searches across large datasets. Qdrant is particularly suited for applications like recommendation systems, semantic search, and AI-driven content discovery, where finding similar items quickly is essential. It supports advanced filtering, scalable indexing, and real-time updates, making it easy to integrate into machine learning workflows.\n\nLearn more from the following resources:\n\n- [@official@Qdrant](https://qdrant.tech/)\n- [@opensource@Qdrant on GitHub](https://github.com/qdrant/qdrant)\n- [@video@Getting started with Qdrant](https://www.youtube.com/watch?v=LRcZ9pbGnno)",
    "rag--implementation@lVhWhZGR558O-ljHobxIi": "# RAG & Implementation\n\nRetrieval-Augmented Generation (RAG) combines information retrieval with language generation to produce more accurate, context-aware responses. It uses two components: a retriever, which searches a database to find relevant information, and a generator, which crafts a response based on the retrieved data. Implementing RAG involves using a retrieval model (e.g., embeddings and vector search) alongside a generative language model (like GPT). The process starts by converting a query into embeddings, retrieving relevant documents from a vector database, and feeding them to the language model, which then generates a coherent, informed response. This approach grounds outputs in real-world data, resulting in more reliable and detailed answers.\n\nLearn more from the following resources:\n\n- [@article@What is RAG?](https://aws.amazon.com/what-is/retrieval-augmented-generation/)\n- [@video@What is Retrieval-Augmented Generation? IBM](https://www.youtube.com/watch?v=T-D1OfcDW1M)\n",
    "rag-usecases@GCn4LGNEtPI0NWYAZCRE-": "# RAG Usecases\n\nRetrieval-Augmented Generation (RAG) enhances applications like chatbots, customer support, and content summarization by combining information retrieval with language generation. It retrieves relevant data from a knowledge base and uses it to generate accurate, context-aware responses, making it ideal for tasks such as question answering, document generation, and semantic search. RAG’s ability to ground outputs in real-world information leads to more reliable and informative results, improving user experience across various domains.\n\nLearn more from the following resources:\n\n- [@article@Retrieval augmented generation use cases: Transforming data into insights](https://www.glean.com/blog/retrieval-augmented-generation-use-cases)\n- [@article@Retrieval Augmented Generation (RAG) – 5 Use Cases](https://theblue.ai/blog/rag-news/)\n- [@video@Introduction to RAG](https://www.youtube.com/watch?v=LmiFeXH-kq8&list=PL-pTHQz4RcBbz78Z5QXsZhe9rHuCs1Jw-)",
    "rag-vs-fine-tuning@qlBEXrbV88e_wAGRwO9hW": "# RAG vs Fine-tuning\n\nRAG (Retrieval-Augmented Generation) and fine-tuning are two approaches to enhancing language models, but they differ in methodology and use cases. Fine-tuning involves training a pre-trained model on a specific dataset to adapt it to a particular task, making it more accurate for that context but limited to the knowledge present in the training data. RAG, on the other hand, combines real-time information retrieval with generation, enabling the model to access up-to-date external data and produce contextually relevant responses. While fine-tuning is ideal for specialized, static tasks, RAG is better suited for dynamic tasks that require real-time, fact-based responses.\n\nLearn more from the following resources:\n\n- [@article@RAG vs Fine Tuning: How to Choose the Right Method](https://www.montecarlodata.com/blog-rag-vs-fine-tuning/)\n- [@article@RAG vs Finetuning — Which Is the Best Tool to Boost Your LLM Application?](https://towardsdatascience.com/rag-vs-finetuning-which-is-the-best-tool-to-boost-your-llm-application-94654b1eaba7)\n- [@video@RAG vs Fine-tuning](https://www.youtube.com/watch?v=00Q0G84kq3M)",
    "rag@9JwWIK0Z2MK8-6EQQJsCO": "# RAG\n\nRetrieval-Augmented Generation (RAG) is an AI approach that combines information retrieval with language generation to create more accurate, contextually relevant outputs. It works by first retrieving relevant data from a knowledge base or external source, then using a language model to generate a response based on that information. This method enhances the accuracy of generative models by grounding their outputs in real-world data, making RAG ideal for tasks like question answering, summarization, and chatbots that require reliable, up-to-date information.\n\nLearn more from the following resources:\n\n- [@article@What is Retrieval Augmented Generation (RAG)? - Datacamp](https://www.datacamp.com/blog/what-is-retrieval-augmented-generation-rag)\n- [@article@What is Retrieval-Augmented Generation? - Google](https://cloud.google.com/use-cases/retrieval-augmented-generation)\n- [@video@What is Retrieval-Augmented Generation? - IBM](https://www.youtube.com/watch?v=T-D1OfcDW1M)",
    "react-prompting@voDKcKvXtyLzeZdx2g3Qn": "# ReAct Prompting\n\nReAct prompting is a technique that combines reasoning and action by guiding language models to think through a problem step-by-step and then take specific actions based on the reasoning. It encourages the model to break down tasks into logical steps (reasoning) and perform operations, such as calling APIs or retrieving information (actions), to reach a solution. This approach helps in scenarios where the model needs to process complex queries, interact with external systems, or handle tasks requiring a sequence of actions, improving the model's ability to provide accurate and context-aware responses.\n\nLearn more from the following resources:\n\n- [@article@ReAct Prompting](https://www.promptingguide.ai/techniques/react)\n- [@article@ReAct Prompting: How We Prompt for High-Quality Results from LLMs](https://www.width.ai/post/react-prompting)",
    "recommendation-systems@HQe9GKy3p0kTUPxojIfSF": "# Recommendation Systems\n\nIn the context of embeddings, recommendation systems use vector representations to capture similarities between items, such as products or content. By converting items and user preferences into embeddings, these systems can measure how closely related different items are based on vector proximity, allowing them to recommend similar products or content based on a user's past interactions. This approach improves recommendation accuracy and efficiency by enabling meaningful, scalable comparisons of complex data.\n\nLearn more from the following resources:\n\n- [@article@What Role does AI Play in Recommendation Systems and Engines?](https://www.algolia.com/blog/ai/what-role-does-ai-play-in-recommendation-systems-and-engines/)\n- [@article@What is a Recommendation Engine?](https://www.ibm.com/think/topics/recommendation-engine)\n",
    "replicate@c0RPhpD00VIUgF4HJgN2T": "# Replicate\n\nReplicate is a platform that allows developers to run machine learning models in the cloud without needing to manage infrastructure. It provides a simple API for deploying and scaling models, making it easy to integrate AI capabilities like image generation, text processing, and more into applications. Users can select from a library of pre-trained models or deploy their own, with the platform handling tasks like scaling, monitoring, and versioning.\n\nLearn more from the following resources:\n\n- [@official@Replicate](https://replicate.com/)\n- [@video@Replicate.com Beginners Tutorial](https://www.youtube.com/watch?v=y0_GE5ErqY8)",
    "retrieval-process@OCGCzHQM2LQyUWmiqe6E0": "# Retrieval Process\n\nThe retrieval process in Retrieval-Augmented Generation (RAG) involves finding relevant information from a large dataset or knowledge base to support the generation of accurate, context-aware responses. When a query is received, the system first converts it into a vector (embedding) and uses this vector to search a database of pre-indexed embeddings, identifying the most similar or relevant data points. Techniques like approximate nearest neighbor (ANN) search are often used to speed up this process.\n\nLearn more from the following resources:\n\n- [@article@What is Retrieval-Augmented Generation (RAG)?](https://cloud.google.com/use-cases/retrieval-augmented-generation)\n- [@article@What Is Retrieval-Augmented Generation, aka RAG?](https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/)",
    "robust-prompt-engineering@qmx6OHqx4_0JXVIv8dASp": "# Robust prompt engineering\n\nRobust prompt engineering involves carefully crafting inputs to guide AI models toward producing accurate, relevant, and reliable outputs. It focuses on minimizing ambiguity and maximizing clarity by providing specific instructions, examples, or structured formats. Effective prompts anticipate potential issues, such as misinterpretation or inappropriate responses, and address them through testing and refinement. This approach enhances the consistency and quality of the model's behavior, making it especially useful for complex tasks like multi-step reasoning, content generation, and interactive systems.\n\nLearn more from the following resources:\n\n- [@article@Building Robust Prompt Engineering Capability](https://aimresearch.co/product/building-robust-prompt-engineering-capability)\n- [@article@Effective Prompt Engineering: A Comprehensive Guide](https://medium.com/@nmurugs/effective-prompt-engineering-a-comprehensive-guide-803160c571ed)",
    "roles-and-responsiblities@K9EiuFgPBFgeRxY4wxAmb": "# Roles and Responsibilities\n\nAI Engineers are responsible for designing, developing, and deploying AI systems that solve real-world problems. Their roles include building machine learning models, implementing data processing pipelines, and integrating AI solutions into existing software or platforms. They work on tasks like data collection, cleaning, and labeling, as well as model training, testing, and optimization to ensure high performance and accuracy. AI Engineers also focus on scaling models for production use, monitoring their performance, and troubleshooting issues. Additionally, they collaborate with data scientists, software developers, and other stakeholders to align AI projects with business goals, ensuring that solutions are reliable, efficient, and ethically sound.\n\nLearn more from the following resources:\n\n- [@article@AI Engineer Job Description](https://resources.workable.com/ai-engineer-job-description)\n- [@article@How To Become an AI Engineer (Plus Job Duties and Skills)](https://www.indeed.com/career-advice/finding-a-job/ai-engineer)",
    "security-and-privacy-concerns@sWBT-j2cRuFqRFYtV_5TK": "# Security and Privacy Concerns\n\nSecurity and privacy concerns in AI revolve around the protection of data and the responsible use of models. Key issues include ensuring that sensitive data, such as personal information, is handled securely during collection, processing, and storage, to prevent unauthorized access and breaches. AI models can also inadvertently expose sensitive data if not properly designed, leading to privacy risks through data leakage or misuse. Additionally, there are concerns about model bias, data misuse, and ensuring transparency in how AI decisions are made.\n\nLearn more from the following resources:\n\n- [@article@Examining Privacy Risks in AI Systems](https://transcend.io/blog/ai-and-privacy)\n- [@video@AI Is Dangerous, but Not for the Reasons You Think | Sasha Luccioni | TED](https://www.youtube.com/watch?v=eXdVDhOGqoE)",
    "semantic-search@eMfcyBxnMY_l_5-8eg6sD": "# Semantic Search\n\nEmbeddings are used for semantic search by converting text, such as queries and documents, into high-dimensional vectors that capture the underlying meaning and context, rather than just exact words. These embeddings represent the semantic relationships between words or phrases, allowing the system to understand the query’s intent and retrieve relevant information, even if the exact terms don’t match.\n\nLearn more from the following resources:\n\n- [@article@What is Semantic Search?](https://www.elastic.co/what-is/semantic-search)\n- [@video@What is Semantic Search? - Cohere](https://www.youtube.com/watch?v=fFt4kR4ntAA)",
    "sentence-transformers@ZV_V6sqOnRodgaw4mzokC": "# Sentence Transformers\n\nSentence Transformers are a type of model designed to generate high-quality embeddings for sentences, allowing them to capture the semantic meaning of text. Unlike traditional word embeddings, which represent individual words, Sentence Transformers understand the context of entire sentences, making them ideal for tasks that require semantic similarity, such as sentence clustering, semantic search, and paraphrase detection. Built on top of transformer models like BERT and RoBERTa, they convert sentences into dense vectors, where similar sentences are placed closer together in vector space.\n\nLearn more from the following resources:\n\n- [@article@What is BERT?](https://h2o.ai/wiki/bert/)\n- [@article@SentenceTransformers Documentation](https://sbert.net/)\n- [@article@Using Sentence Transformers at Hugging Face](https://huggingface.co/docs/hub/sentence-transformers)\n",
    "speech-to-text@jQX10XKd_QM5wdQweEkVJ": "# Speech-to-Text\n\nIn the context of multimodal AI, speech-to-text technology converts spoken language into written text, enabling seamless integration with other data types like images and text. This allows AI systems to process audio input and combine it with visual or textual information, enhancing applications such as virtual assistants, interactive chatbots, and multimedia content analysis. For example, a multimodal AI can transcribe a video’s audio while simultaneously analyzing on-screen visuals and text, providing richer and more context-aware insights.\n\nLearn more from the following resources:\n\n- [@article@What is Speech to Text?](https://aws.amazon.com/what-is/speech-to-text/)\n- [@article@Turn Speech into Text using Google AI](https://cloud.google.com/speech-to-text)\n- [@article@How is Speech to Text Used?](https://h2o.ai/wiki/speech-to-text/)\n",
    "supabase@9kT7EEQsbeD2WDdN9ADx7": "# Supabase\n\nSupabase Vector is an extension of the Supabase platform, specifically designed for AI and machine learning applications that require vector operations. It leverages PostgreSQL's pgvector extension to provide efficient vector storage and similarity search capabilities. This makes Supabase Vector particularly useful for applications involving embeddings, semantic search, and recommendation systems. With Supabase Vector, developers can store and query high-dimensional vector data alongside regular relational data, all within the same PostgreSQL database.\n\nLearn more from the following resources:\n\n- [@official@Supabase Vector](https://supabase.com/docs/guides/ai)\n- [@video@Supabase Vector: The Postgres Vector database](https://www.youtube.com/watch?v=MDxEXKkxf2Q)\n",
    "text-to-speech@GCERpLz5BcRtWPpv-asUz": "# Text-to-Speech\n\nIn the context of multimodal AI, text-to-speech (TTS) technology converts written text into natural-sounding spoken language, allowing AI systems to communicate verbally. When integrated with other modalities, such as visual or interactive elements, TTS can enhance user experiences in applications like virtual assistants, educational tools, and accessibility features. For example, a multimodal AI could read aloud text from an on-screen document while highlighting relevant sections, or narrate information about objects recognized in an image. By combining TTS with other forms of data processing, multimodal AI creates more engaging, accessible, and interactive systems for users.\n\nLearn more from the following resources:\n\n- [@article@What is Text-to-Speech?](https://aws.amazon.com/polly/what-is-text-to-speech/)\n- [@article@From Text to Speech: The Evolution of Synthetic Voices](https://ignitetech.ai/about/blogs/text-speech-evolution-synthetic-voices)",
    "token-counting@FjV3oD7G2Ocq5HhUC17iH": "# Token Counting\n\nToken counting refers to tracking the number of tokens processed during interactions with language models, including both input and output text. Tokens are units of text that can be as short as a single character or as long as a word, and models like GPT process text by splitting it into these tokens. Knowing how many tokens are used is crucial because the API has token limits (e.g., 4,096 for GPT-3 and up to 32,768 for some versions of GPT-4), and costs are typically calculated based on the total number of tokens processed.\n\nLearn more from the following resources:\n\n- [@official@OpenAI Tokenizer Tool](https://platform.openai.com/tokenizer)\n- [@article@How to count tokens with Tiktoken](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken)\n",
    "training@xostGgoaYkqMO28iN2gx8": "# Training\n\nTraining refers to the process of teaching a machine learning model to recognize patterns and make predictions by exposing it to a dataset. During training, the model learns from the data by adjusting its internal parameters to minimize errors between its predictions and the actual outcomes. This process involves iteratively feeding the model with input data, comparing its outputs to the correct answers, and refining its predictions through techniques like gradient descent. The goal is to enable the model to generalize well so that it can make accurate predictions on new, unseen data.\n\nLearn more from the following resources:\n\n- [@article@What is Model Training?](https://oden.io/glossary/model-training/)\n- [@article@Machine learning model training: What it is and why it’s important](https://domino.ai/blog/what-is-machine-learning-model-training)\n- [@article@Training ML Models - Amazon](https://docs.aws.amazon.com/machine-learning/latest/dg/training-ml-models.html)\n",
    "transformersjs@bGLrbpxKgENe2xS1eQtdh": "# Transformers.js\n\nTransformers.js is a JavaScript library that enables transformer models, like those from Hugging Face, to run directly in the browser or Node.js, without needing cloud services. It supports tasks such as text generation, sentiment analysis, and translation within web apps or server-side scripts. Using WebAssembly (Wasm) and efficient JavaScript, Transformers.js offers powerful NLP capabilities with low latency, enhanced privacy, and offline functionality, making it ideal for real-time, interactive applications where local processing is essential for performance and security.\n\nLearn more from the following resources:\n\n- [@official@Transformers.js on Hugging Face](https://huggingface.co/docs/transformers.js/en/index)\n- [@video@How Transformer.js Can Help You Create Smarter AI In Your Browser](https://www.youtube.com/watch?v=MNJHu9zjpqg)\n",
    "using-sdks-directly@WZVW8FQu6LyspSKm1C_sl": "# Using SDKs Directly\n\nWhile tools like Langchain and LlamaIndex make it easy to implement RAG, you don't have to necessarily learn and use them. If you know about the different steps of implementing RAG you can simply do it all yourself e.g. do the chunking using `@langchain/textsplitters` package, create embeddings using any LLM e.g. use OpenAI Embedding API through their SDK, save the embeddings to any vector database e.g. if you are using Supabase Vector DB, you can use their SDK and similarly you can use the relevant SDKs for the rest of the steps as well.\n\nLearn more from the following resources:\n\n- [@official@Langchain Text Splitter Package](https://www.npmjs.com/package/@langchain/textsplitters)\n- [@official@OpenAI Embedding API](https://platform.openai.com/docs/guides/embeddings)\n- [@official@Supabase AI & Vector Documentation](https://supabase.com/docs/guides/ai)\n",
    "vector-database@zZA1FBhf1y4kCoUZ-hM4H": "# Vector Database\n\nWhen implementing Retrieval-Augmented Generation (RAG), a vector database is used to store and efficiently retrieve embeddings, which are vector representations of data like documents, images, or other knowledge sources. During the RAG process, when a query is made, the system converts it into an embedding and searches the vector database for the most relevant, similar embeddings (e.g., related documents or snippets). These retrieved pieces of information are then fed to a generative model, which uses them to produce a more accurate, context-aware response.\n\nLearn more from the following resources:\n\n- [@article@How to Implement Graph RAG Using Knowledge Graphs and Vector Databases](https://towardsdatascience.com/how-to-implement-graph-rag-using-knowledge-graphs-and-vector-databases-60bb69a22759)\n- [@article@Retrieval Augmented Generation (RAG) with Vector Databases: Expanding AI Capabilities](https://objectbox.io/retrieval-augmented-generation-rag-with-vector-databases-expanding-ai-capabilities/)\n",
    "vector-databases@LnQ2AatMWpExUHcZhDIPd": "# Vector Databases\n\nVector databases are specialized systems designed to store, index, and retrieve high-dimensional vectors, often used as embeddings that represent data like text, images, or audio. Unlike traditional databases that handle structured data, vector databases excel at managing unstructured data by enabling fast similarity searches, where vectors are compared to find those that are most similar to a query. This makes them essential for tasks like semantic search, recommendation systems, and content discovery, where understanding relationships between items is crucial. Vector databases use indexing techniques such as approximate nearest neighbor (ANN) search to efficiently handle large datasets, ensuring quick and accurate retrieval even at scale.\n\nLearn more from the following resources:\n\n- [@article@Vector Databases](https://developers.cloudflare.com/vectorize/reference/what-is-a-vector-database/)\n- [@article@What are Vector Databases?](https://www.mongodb.com/resources/basics/databases/vector-databases)\n",
    "vector-databases@tt9u3oFlsjEMfPyojuqpc": "# Vector Databases\n\nVector databases are systems specialized in storing, indexing, and retrieving high-dimensional vectors, often used as embeddings for data like text, images, or audio. Unlike traditional databases, they excel at managing unstructured data by enabling fast similarity searches, where vectors are compared to find the closest matches. This makes them essential for tasks like semantic search, recommendation systems, and content discovery. Using techniques like approximate nearest neighbor (ANN) search, vector databases handle large datasets efficiently, ensuring quick and accurate retrieval even at scale.\n\nLearn more from the following resources:\n\n- [@article@Vector Databases](https://developers.cloudflare.com/vectorize/reference/what-is-a-vector-database/)\n- [@article@What are Vector Databases?](https://www.mongodb.com/resources/basics/databases/vector-databases)\n",
    "video-understanding@TxaZCtTCTUfwCxAJ2pmND": "# Video Understanding\n\nVideo understanding with multimodal AI involves analyzing and interpreting both visual and audio content to provide a more comprehensive understanding of videos. Common use cases include video summarization, where AI extracts key scenes and generates summaries; content moderation, where the system detects inappropriate visuals or audio; and video indexing for easier search and retrieval of specific moments within a video. Other applications include enhancing video-based recommendations, security surveillance, and interactive entertainment, where video and audio are processed together for real-time user interaction.\n\nLearn more from the following resources:\n\n- [@article@Video Understanding](https://dl.acm.org/doi/10.1145/3503161.3551600)\n- [@opensource@Awesome LLM for Video Understanding](https://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding)\n",
    "weaviate@VgUnrZGKVjAAO4n_llq5-": "# Weaviate\n\nWeaviate is an open-source vector database that allows users to store, search, and manage high-dimensional vectors, often used for tasks like semantic search and recommendation systems. It enables efficient similarity searches by converting data (like text, images, or audio) into embeddings and indexing them for fast retrieval. Weaviate also supports integrating external data sources and schemas, making it easy to combine structured and unstructured data.\n\nLearn more from the following resources:\n\n- [@official@Weaviate](https://weaviate.io/)\n- [@video@Advanced AI Agents with RAG](https://www.youtube.com/watch?v=UoowC-hsaf0&list=PLTL2JUbrY6tVmVxY12e6vRDmY-maAXzR1)\n",
    "what-are-embeddings@--ig0Ume_BnXb9K2U7HJN": "# What are Embeddings\n\nEmbeddings are dense, numerical vector representations of data, such as words, sentences, images, or audio, that capture their semantic meaning and relationships. By converting data into fixed-length vectors, embeddings allow machine learning models to process and understand the data more effectively. For example, word embeddings represent similar words with similar vectors, enabling tasks like semantic search, recommendation systems, and clustering. Embeddings make it easier to compare, search, and analyze complex, unstructured data by mapping similar items close together in a high-dimensional space.\n\nVisit the following resources to learn more:\n\n- [@official@Introducing Text and Code Embeddings](https://openai.com/index/introducing-text-and-code-embeddings/)\n- [@article@What are Embeddings](https://www.cloudflare.com/learning/ai/what-are-embeddings/)\n",
    "what-is-an-ai-engineer@GN6SnI7RXIeW8JeD-qORW": "# What is an AI Engineer?\n\nAI engineers are professionals who specialize in designing, developing, and implementing artificial intelligence (AI) systems. Their work is essential in various industries, as they create applications that enable machines to perform tasks that typically require human intelligence, such as problem-solving, learning, and decision-making.\n\nVisit the following resources to learn more:\n\n- [@article@How to Become an AI Engineer: Duties, Skills, and Salary](https://www.simplilearn.com/tutorials/artificial-intelligence-tutorial/how-to-become-an-ai-engineer)\n- [@article@AI Engineers: What they do and how to become one](https://www.techtarget.com/whatis/feature/How-to-become-an-artificial-intelligence-engineer)\n- [@course@AI For Everyone](https://www.coursera.org/learn/ai-for-everyone)\n",
    "whisper-api@OTBd6cPUayKaAM-fLWdSt": "# Whisper API\n\nThe Whisper API by OpenAI enables developers to integrate speech-to-text capabilities into their applications. It uses OpenAI's Whisper model, a powerful speech recognition system, to convert spoken language into accurate, readable text. The API supports multiple languages and can handle various accents, making it ideal for tasks like transcription, voice commands, and automated captions. With the ability to process audio in real time or from pre-recorded files, the Whisper API simplifies adding robust speech recognition features to applications, enhancing accessibility and enabling new interactive experiences.\n\nLearn more from the following resources:\n\n- [@official@OpenAI Whisper](https://openai.com/index/whisper/)\n- [@opensource@Whisper on GitHub](https://github.com/openai/whisper)\n",
    "writing-prompts@9-5DYeOnKJq9XvEMWP45A": "# Writing Prompts\n\nPrompts for the OpenAI API are carefully crafted inputs designed to guide the language model in generating specific, high-quality content. These prompts can be used to direct the model to create stories, articles, dialogue, or even detailed responses on particular topics. Effective prompts set clear expectations by providing context, specifying the format, or including examples, such as \"Write a short sci-fi story about a future where humans can communicate with animals,\" or \"Generate a detailed summary of the key benefits of using renewable energy.\" Well-designed prompts help ensure that the API produces coherent, relevant, and creative outputs, making it easier to achieve desired results across various applications.\n\nLearn more from the following resources:\n\n- [@roadmap@Visit Dedicated Prompt Engineering Roadmap](https://roadmap.sh/prompt-engineering)\n- [@article@How to Write AI prompts](https://www.descript.com/blog/article/how-to-write-ai-prompts)\n- [@article@Prompt Engineering Guide](https://www.promptingguide.ai/)\n"
  }
}